{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LR = 1e-3\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x if (x.ndim == 3) and (x.shape[0] == 3) else x.repeat(3, 1, 1)),\n",
    "    transforms.Resize((32, 32), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "dataset_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataset_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b90aa4fee7745cd805c5621f5e92b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN().to(DEVICE)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "prog_bar = tqdm(total=EPOCHS * len(train_loader))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y_hat) in enumerate(train_loader):\n",
    "        x, y_hat = x.to(DEVICE), y_hat.to(DEVICE)\n",
    "        y = model(x)\n",
    "        l = loss(y, y_hat)\n",
    "        optimiser.zero_grad()\n",
    "        l.backward()\n",
    "        optimiser.step()\n",
    "        prog_bar.update(1)\n",
    "        if step % 100 == 0:\n",
    "            prog_bar.set_description(f'epoch: {epoch} loss: {l.detach().cpu().item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.982421875\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for x, y_hat in test_loader:\n",
    "    x, y_hat = x.to(DEVICE), y_hat.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    pred = torch.argmax(y, dim=-1)\n",
    "    accuracy = (y_hat == pred).to(torch.float).mean().cpu().item()\n",
    "    accuracies.append(accuracy)\n",
    "print(f'Accuracy: {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "student = SmallMLP().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 10\n",
    "ALPHA = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010d2033739245d0ac14c05e87eeaf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/slg3jsps2ml2ptbzd0kp6sp40000gn/T/ipykernel_97076/3581741736.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher_dist = F.softmax(teacher_logits / TEMPERATURE)\n",
      "/var/folders/kp/slg3jsps2ml2ptbzd0kp6sp40000gn/T/ipykernel_97076/3581741736.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student_dist = F.softmax(student_logits / TEMPERATURE)\n",
      "/Users/thomasfoster/Documents/meta_representations/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "loss_ce = nn.CrossEntropyLoss()\n",
    "loss_kld = nn.KLDivLoss()\n",
    "prog_bar = tqdm(total=EPOCHS * len(train_loader))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y_hat) in enumerate(train_loader):\n",
    "        x, y_hat = x.to(DEVICE), y_hat.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = model(x)\n",
    "            teacher_dist = F.softmax(teacher_logits / TEMPERATURE)\n",
    "\n",
    "        student_logits = student(x)\n",
    "        student_dist = F.softmax(student_logits / TEMPERATURE)\n",
    "\n",
    "        l = ALPHA*loss_ce(student_logits, y_hat) + (1-ALPHA)*loss_kld(student_dist, teacher_dist)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        l.backward()\n",
    "        optimiser.step()\n",
    "        prog_bar.update(1)\n",
    "        if step % 100 == 0:\n",
    "            prog_bar.set_description(f'epoch: {epoch} loss: {l.detach().cpu().item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86103515625\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for x, y_hat in test_loader:\n",
    "    x, y_hat = x.to(DEVICE), y_hat.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y = student(x)\n",
    "    pred = torch.argmax(y, dim=-1)\n",
    "    accuracy = (y_hat == pred).to(torch.float).mean().cpu().item()\n",
    "    accuracies.append(accuracy)\n",
    "print(f'Accuracy: {sum(accuracies)/len(accuracies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    n_params = 0\n",
    "    for param in model.parameters():\n",
    "        n_params += param.nelement()\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62006\n",
      "30950\n"
     ]
    }
   ],
   "source": [
    "print(n_params(model))\n",
    "print(n_params(student))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
